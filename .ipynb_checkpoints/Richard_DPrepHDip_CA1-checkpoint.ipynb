{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21f223f",
   "metadata": {},
   "source": [
    "# 1. Quick Preview of the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670b1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv(\"aps_failure_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da48f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf597d",
   "metadata": {},
   "source": [
    "# 2. Characterisation of the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b051a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv(\"aps_failure_set.csv\", na_values=\"na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c7b12",
   "metadata": {},
   "source": [
    "## 2.1 Size, number of attributes, number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9641537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 60000\n",
      "Number of Columns (Attributes): 171\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_cols = dataframe.shape\n",
    "\n",
    "print(\"Number of Rows:\", num_rows)\n",
    "print(\"Number of Columns (Attributes):\", num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496024ec",
   "metadata": {},
   "source": [
    "- Dataset is defined by means of a number of rows representing observations that determine the size of the dataset being analyzed. It gives us an idea of the number of instances that describe each variable in our data set.\n",
    "\n",
    "- By knowing how many columns a data set has, one easily understands its dimensionality. Every column refers to a variable, attribute or a feature. Therefore, the number of attributes tells us the amount of details available in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f263d84",
   "metadata": {},
   "source": [
    "## 2.2 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9b3afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attributes with Missing Values: 169\n"
     ]
    }
   ],
   "source": [
    "missing_values = dataframe.isna().sum()\n",
    "attributes_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "print(\"Number of Attributes with Missing Values:\", attributes_with_missing_values.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f445870",
   "metadata": {},
   "source": [
    "## 2.3 Percentage of Missing Values for Each Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "missing_percentage = dataframe.isna().mean() * 100\n",
    "\n",
    "x_bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "y_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170]\n",
    "\n",
    "plt.hist(missing_percentage, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution of Missing Value Percentages\")\n",
    "plt.xlabel(\"Missing Percentage Range\")\n",
    "plt.ylabel(\"Number of Attributes\")\n",
    "plt.xticks(x_bins)\n",
    "plt.yticks(y_bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0688047",
   "metadata": {},
   "source": [
    "- Missing values in data can be easily identified if the attributes are listed. Analysis and modeling may be compromised by missing data, and therefore it affects the validity.\n",
    "\n",
    "- We determine the degree of missing data by considering the number of attributes that lack values as well as the missing frequency for each attribute. Missing data in attributes that involve many missing values is handled via filling them using some sort of imputation or removal technique.\n",
    "\n",
    "- It is important to tackle the issue of missing values as it directly affects decision-making concerning optimal data pretreatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f38890",
   "metadata": {},
   "source": [
    "# 3. Data Preparation and EDA Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3e9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_na_and_zeros = dataframe.drop(columns=\"class\")\n",
    "categorical_target = dataframe[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b18d4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 170 entries, aa_000 to eg_000\n",
      "dtypes: float64(169), int64(1)\n",
      "memory usage: 77.8 MB\n"
     ]
    }
   ],
   "source": [
    "features_with_na_and_zeros.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c11ee",
   "metadata": {},
   "source": [
    "- We have numerical values for all columns on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0a10fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for target: ['neg' 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values for target:\", categorical_target.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8eb039",
   "metadata": {},
   "source": [
    "## 3.1 Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2cd1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features_with_na = features_with_na_and_zeros.replace(0, np.nan)\n",
    "features = features_with_na.fillna(features_with_na.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6a82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_dummy_target = pd.get_dummies(categorical_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bdfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_target = numerical_dummy_target.drop(\"neg\", axis=1)\n",
    "target = numerical_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a094a",
   "metadata": {},
   "source": [
    "## 3.2 PCA to Establish Minimum Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21bf63e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79113df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'green',\n",
    "        'weight': 'normal',\n",
    "        'size': 14}\n",
    "\n",
    "text_line_1 = '\"We will take 5 features as'\n",
    "text_line_2 = ' the minimum number of features'\n",
    "text_line_3 = ' needed for retaining 99.5% variance\"'\n",
    "\n",
    "text_line_x_position = 15\n",
    "\n",
    "text_line_1_y_position = 0.998\n",
    "text_line_2_y_position = 0.9965\n",
    "text_line_3_y_position = 0.995\n",
    "\n",
    "x_bins = [0, 5, 25, 50, 100]\n",
    "y_bins = [0.975, 0.980, 0.985, 0.990, .995, 1.0]\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('PCA to Establish Minimum Number of Features')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Percentage of Variance Explained');\n",
    "plt.xticks(x_bins)\n",
    "plt.yticks(y_bins)\n",
    "plt.text(text_line_x_position, text_line_1_y_position, text_line_1, style='italic', fontdict=font)\n",
    "plt.text(text_line_x_position, text_line_2_y_position, text_line_2, style='italic', fontdict=font)\n",
    "plt.text(text_line_x_position, text_line_3_y_position, text_line_3, style='italic', fontdict=font)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7bb1d",
   "metadata": {},
   "source": [
    "## 3.3 Implement PCA to Dimensionally Reduce the Data to the Number of Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ea944a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5068fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(5)\n",
    "features_pca = pca.fit_transform(features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc1758c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.414146</td>\n",
       "      <td>-0.850533</td>\n",
       "      <td>-1.779836</td>\n",
       "      <td>0.607394</td>\n",
       "      <td>0.005878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.656664</td>\n",
       "      <td>-0.143985</td>\n",
       "      <td>-0.180514</td>\n",
       "      <td>0.212077</td>\n",
       "      <td>-0.106131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.295238</td>\n",
       "      <td>0.212565</td>\n",
       "      <td>-0.212043</td>\n",
       "      <td>-0.669019</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.307475</td>\n",
       "      <td>-0.689051</td>\n",
       "      <td>0.178176</td>\n",
       "      <td>-0.527675</td>\n",
       "      <td>0.468287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107299</td>\n",
       "      <td>0.130659</td>\n",
       "      <td>-0.682754</td>\n",
       "      <td>-0.103681</td>\n",
       "      <td>-0.337263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C1        C2        C3        C4        C5\n",
       "0  2.414146 -0.850533 -1.779836  0.607394  0.005878\n",
       "1 -0.656664 -0.143985 -0.180514  0.212077 -0.106131\n",
       "2 -1.295238  0.212565 -0.212043 -0.669019  0.413000\n",
       "3 -3.307475 -0.689051  0.178176 -0.527675  0.468287\n",
       "4  0.107299  0.130659 -0.682754 -0.103681 -0.337263"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_features = pd.DataFrame(features_pca, columns =['C1', 'C2', 'C3', 'C4', 'C5'])\n",
    "reduced_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(reduced_features.iloc[:, 0:4], alpha=0.5, figsize=(10, 8))\n",
    "plt.suptitle(\"Pairwise Scatter Plots for Attributes 0 to 3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target[\"pos\"].value_counts().plot(kind=\"bar\", color=\"lightblue\")\n",
    "plt.title(\"Distribution of Target Attribute\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb8ada",
   "metadata": {},
   "source": [
    "# 4. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3be3cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = reduced_features\n",
    "Y = target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167147df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdc27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d13d39",
   "metadata": {},
   "source": [
    "# 5. Curve of Dimensionality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9ab3e",
   "metadata": {},
   "source": [
    "The problems experienced when many attributes and features are incorporated into our data is commonly referred to as the Curse of Dimensionality. We had 170 features which translates to many attributes and this posed some challenges due that.\n",
    "\n",
    "Secondly, the project grew in terms of its computational requirements. For instance, the attribute is an extensive data set and hence our analysis took much time as our computers needed to perform more computations thus slowing us down.\n",
    "\n",
    "Further, interpreting our information became more challenging. It was difficult to see connections or relations among such a number of features. These interactions could not be shown through traditional plots and graphs.\n",
    "\n",
    "Furthermore, as data became bigger, it also raised a risk of introducing irrelevant information that may bias our model.\n",
    "\n",
    "In order to deal with these problems we downsized the dataset made up of 170 features into 5 using PCA method to decrease attribute count while preserving key data, rendering our project less complicated and intelligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2b7ed",
   "metadata": {},
   "source": [
    "# 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab9494",
   "metadata": {},
   "source": [
    "Finally, in brief, the data characterisation process helped us discover any missing values. We focused on curving dimensions and used PCA, which helped us address dimensionality problems. After that, we did some comparative studies between machine-learning models. Success of our project lied upon dealing with the complexity involved during the use of multi-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
